{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "9KSAUkOraNzN"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/jan/miniconda3/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import random\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import copy as cp\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import AllChem\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import gpytorch\n",
        "from gpytorch.kernels import Kernel\n",
        "from gpytorch.mlls import ExactMarginalLogLikelihood\n",
        "from botorch.fit import fit_gpytorch_model\n",
        "from gpytorch.means import ConstantMean\n",
        "from botorch.models import SingleTaskGP\n",
        "from gpytorch.kernels import ScaleKernel\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "from botorch.exceptions import InputDataWarning\n",
        "import warnings\n",
        "import random\n",
        "# To ignore a specific UserWarning about tensor construction\n",
        "warnings.filterwarnings('ignore', message='.*To copy construct from a tensor.*', category=UserWarning)\n",
        "\n",
        "# To ignore a specific InputDataWarning about input data not being standardized\n",
        "warnings.filterwarnings('ignore', message='.*Input data is not standardized.*', category=UserWarning)\n",
        "\n",
        "warnings.filterwarnings('ignore', category=InputDataWarning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "_IhkT30ubJfD"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<>:15: SyntaxWarning: invalid escape sequence '\\c'\n",
            "<>:15: SyntaxWarning: invalid escape sequence '\\c'\n",
            "/var/folders/gd/bfshjrxn51l1th3z7_z71f5w0000gn/T/ipykernel_72921/2981832511.py:15: SyntaxWarning: invalid escape sequence '\\c'\n",
            "  \"\"\"\n"
          ]
        }
      ],
      "source": [
        "# Set device: Apple/NVIDIA/CPU\n",
        "if torch.backends.mps.is_available():\n",
        "    device = torch.device(\"mps\")\n",
        "elif torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "dtype = torch.float\n",
        "\n",
        "\n",
        "def batch_tanimoto_sim(\n",
        "    x1: torch.Tensor, x2: torch.Tensor, eps: float = 1e-6\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Tanimoto similarity between two batched tensors, across last 2 dimensions.\n",
        "    eps argument ensures numerical stability if all zero tensors are added. Tanimoto similarity is proportional to:\n",
        "\n",
        "    (<x, y>) / (||x||^2 + ||y||^2 - <x, y>)\n",
        "\n",
        "    where x and y may be bit or count vectors or in set notation:\n",
        "\n",
        "    |A \\cap B | / |A| + |B| - |A \\cap B |\n",
        "\n",
        "    Args:\n",
        "        x1: `[b x n x d]` Tensor where b is the batch dimension\n",
        "        x2: `[b x m x d]` Tensor\n",
        "        eps: Float for numerical stability. Default value is 1e-6\n",
        "    Returns:\n",
        "        Tensor denoting the Tanimoto similarity.\n",
        "    #from here https://github.com/leojklarner/gauche/blob/main/gauche/kernels/fingerprint_kernels/tanimoto_kernel.py\n",
        "    \"\"\"\n",
        "\n",
        "    if x1.ndim < 2 or x2.ndim < 2:\n",
        "        raise ValueError(\"Tensors must have a batch dimension\")\n",
        "\n",
        "    dot_prod = torch.matmul(x1, torch.transpose(x2, -1, -2))\n",
        "    x1_norm = torch.sum(x1**2, dim=-1, keepdims=True)\n",
        "    x2_norm = torch.sum(x2**2, dim=-1, keepdims=True)\n",
        "\n",
        "    tan_similarity = (dot_prod + eps) / (\n",
        "        eps + x1_norm + torch.transpose(x2_norm, -1, -2) - dot_prod\n",
        "    )\n",
        "\n",
        "    return tan_similarity.clamp_min_(\n",
        "        0\n",
        "    )  # zero out negative values for numerical stability\n",
        "\n",
        "\n",
        "class TanimotoKernel(Kernel):\n",
        "    r\"\"\"\n",
        "     Computes a covariance matrix based on the Tanimoto kernel\n",
        "     between inputs :math:`\\mathbf{x_1}` and :math:`\\mathbf{x_2}`:\n",
        "\n",
        "     .. math::\n",
        "\n",
        "    \\begin{equation*}\n",
        "     k_{\\text{Tanimoto}}(\\mathbf{x}, \\mathbf{x'}) = \\frac{\\langle\\mathbf{x},\n",
        "     \\mathbf{x'}\\rangle}{\\left\\lVert\\mathbf{x}\\right\\rVert^2 + \\left\\lVert\\mathbf{x'}\\right\\rVert^2 -\n",
        "     \\langle\\mathbf{x}, \\mathbf{x'}\\rangle}\n",
        "    \\end{equation*}\n",
        "\n",
        "    .. note::\n",
        "\n",
        "     This kernel does not have an `outputscale` parameter. To add a scaling parameter,\n",
        "     decorate this kernel with a :class:`gpytorch.test_kernels.ScaleKernel`.\n",
        "\n",
        "     Example:\n",
        "         >>> x = torch.randint(0, 2, (10, 5))\n",
        "         >>> # Non-batch: Simple option\n",
        "         >>> covar_module = gpytorch.kernels.ScaleKernel(TanimotoKernel())\n",
        "         >>> covar = covar_module(x)  # Output: LazyTensor of size (10 x 10)\n",
        "         >>>\n",
        "         >>> batch_x = torch.randint(0, 2, (2, 10, 5))\n",
        "         >>> # Batch: Simple option\n",
        "         >>> covar_module = gpytorch.kernels.ScaleKernel(TanimotoKernel())\n",
        "         >>> covar = covar_module(batch_x)  # Output: LazyTensor of size (2 x 10 x 10)\n",
        "    \"\"\"\n",
        "\n",
        "    is_stationary = False\n",
        "    has_lengthscale = False\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        super(TanimotoKernel, self).__init__(**kwargs)\n",
        "\n",
        "    def forward(self, x1, x2, diag=False, **params):\n",
        "        if diag:\n",
        "            assert x1.size() == x2.size() and torch.equal(x1, x2)\n",
        "            return torch.ones(\n",
        "                *x1.shape[:-2], x1.shape[-2], dtype=x1.dtype, device=x1.device\n",
        "            )\n",
        "        else:\n",
        "            return self.covar_dist(x1, x2, **params)\n",
        "\n",
        "    def covar_dist(\n",
        "        self,\n",
        "        x1,\n",
        "        x2,\n",
        "        last_dim_is_batch=False,\n",
        "        **params,\n",
        "    ):\n",
        "        r\"\"\"This is a helper method for computing the bit vector similarity between\n",
        "        all pairs of points in x1 and x2.\n",
        "\n",
        "        Args:\n",
        "            :attr:`x1` (Tensor `n x d` or `b1 x ... x bk x n x d`):\n",
        "                First set of data.\n",
        "            :attr:`x2` (Tensor `m x d` or `b1 x ... x bk x m x d`):\n",
        "                Second set of data.\n",
        "            :attr:`last_dim_is_batch` (tuple, optional):\n",
        "                Is the last dimension of the data a batch dimension or not?\n",
        "\n",
        "        Returns:\n",
        "            (:class:`Tensor`, :class:`Tensor) corresponding to the distance matrix between `x1` and `x2`.\n",
        "            The shape depends on the kernel's mode\n",
        "            * `diag=False`\n",
        "            * `diag=False` and `last_dim_is_batch=True`: (`b x d x n x n`)\n",
        "            * `diag=True`\n",
        "            * `diag=True` and `last_dim_is_batch=True`: (`b x d x n`)\n",
        "        \"\"\"\n",
        "        if last_dim_is_batch:\n",
        "            x1 = x1.transpose(-1, -2).unsqueeze(-1)\n",
        "            x2 = x2.transpose(-1, -2).unsqueeze(-1)\n",
        "\n",
        "        return batch_tanimoto_sim(x1, x2)\n",
        "\n",
        "\n",
        "def update_model(\n",
        "    X,\n",
        "    y,\n",
        "    bounds_norm,\n",
        "    kernel_type=\"Tanimoto\",\n",
        "    fit_y=True,\n",
        "    FIT_METHOD=True,\n",
        "    surrogate=\"GP\",\n",
        "):\n",
        "    \"\"\"\n",
        "    Update and return a Gaussian Process (GP) model with new training data.\n",
        "    This function configures and optimizes the GP model based on the provided parameters.\n",
        "\n",
        "    Args:\n",
        "        X (numpy.ndarray): The training data, typically feature vectors.\n",
        "        y (numpy.ndarray): The corresponding labels or values for the training data.\n",
        "        bounds_norm (numpy.ndarray): Normalization bounds for the training data.\n",
        "        kernel_type (str, optional): Type of kernel to be used in the GP model. Default is \"Tanimoto\".\n",
        "        fit_y (bool, optional): Flag to indicate if the output values (y) should be fitted. Default is True.\n",
        "        FIT_METHOD (bool, optional): Flag to indicate the fitting method to be used. Default is True.\n",
        "        surrogate (str, optional): Type of surrogate model to be used. Default is \"GP\".\n",
        "\n",
        "    Returns:\n",
        "        model (botorch.models.gpytorch.GP): The updated GP model, fitted with the provided training data.\n",
        "        scaler_y (TensorStandardScaler): The scaler used for the labels, which can be applied for future data normalization.\n",
        "\n",
        "    Notes:\n",
        "        The function initializes a GP model with specified kernel and fitting methods, then fits the model to the provided data.\n",
        "        The 'bounds_norm' parameter is used for normalizing the training data within the GP model.\n",
        "        The 'fit_y' and 'FIT_METHOD' parameters control the fitting behavior of the model.\n",
        "    \"\"\"\n",
        "\n",
        "    GP_class = Surrogate_Model(\n",
        "        kernel_type=kernel_type,\n",
        "        bounds_norm=bounds_norm,\n",
        "        fit_y=fit_y,\n",
        "        FIT_METHOD=FIT_METHOD,\n",
        "        surrogate=surrogate,\n",
        "    )\n",
        "    model = GP_class.fit(X, y)\n",
        "\n",
        "    return model, GP_class.scaler_y\n",
        "\n",
        "\n",
        "class TensorStandardScaler:\n",
        "    \"\"\"\n",
        "    StandardScaler for tensors that standardizes features by removing the mean\n",
        "    and scaling to unit variance, as defined in BoTorch.\n",
        "\n",
        "    Attributes:\n",
        "        dim (int): The dimension over which to compute the mean and standard deviation.\n",
        "        epsilon (float): A small constant to avoid division by zero in case of a zero standard deviation.\n",
        "        mean (Tensor, optional): The mean value computed in the `fit` method. None until `fit` is called.\n",
        "        std (Tensor, optional): The standard deviation computed in the `fit` method. None until `fit` is called.\n",
        "\n",
        "    Args:\n",
        "        dim (int): The dimension over which to standardize the data. Default is -2.\n",
        "        epsilon (float): A small constant to avoid division by zero. Default is 1e-9.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dim: int = -2, epsilon: float = 1e-9):\n",
        "        self.dim = dim\n",
        "        self.epsilon = epsilon\n",
        "        self.mean = None\n",
        "        self.std = None\n",
        "\n",
        "    def fit(self, Y):\n",
        "        if isinstance(Y, np.ndarray):\n",
        "            Y = torch.from_numpy(Y).float()\n",
        "        self.mean = Y.mean(dim=self.dim, keepdim=True)\n",
        "        self.std = Y.std(dim=self.dim, keepdim=True)\n",
        "        self.std = self.std.where(\n",
        "            self.std >= self.epsilon, torch.full_like(self.std, 1.0)\n",
        "        )\n",
        "\n",
        "    def transform(self, Y):\n",
        "        if self.mean is None or self.std is None:\n",
        "            raise ValueError(\n",
        "                \"Mean and standard deviation not initialized, run `fit` method first.\"\n",
        "            )\n",
        "        original_type = None\n",
        "        if isinstance(Y, np.ndarray):\n",
        "            original_type = np.ndarray\n",
        "            Y = torch.from_numpy(Y).float()\n",
        "        Y_transformed = (Y - self.mean) / self.std\n",
        "        if original_type is np.ndarray:\n",
        "            return Y_transformed.numpy()\n",
        "        else:\n",
        "            return Y_transformed\n",
        "\n",
        "    def fit_transform(self, Y):\n",
        "        self.fit(Y)\n",
        "        return self.transform(Y)\n",
        "\n",
        "    def inverse_transform(self, Y):\n",
        "        if self.mean is None or self.std is None:\n",
        "            raise ValueError(\n",
        "                \"Mean and standard deviation not initialized, run `fit` method first.\"\n",
        "            )\n",
        "        original_type = None\n",
        "        if isinstance(Y, np.ndarray):\n",
        "            original_type = np.ndarray\n",
        "            Y = torch.from_numpy(Y).float()\n",
        "        Y_inv_transformed = (Y * self.std) + self.mean\n",
        "        if original_type is np.ndarray:\n",
        "            return Y_inv_transformed.numpy()\n",
        "        else:\n",
        "            return Y_inv_transformed\n",
        "\n",
        "\n",
        "class Surrogate_Model:\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        kernel_type=\"Tanimoto\",\n",
        "        bounds_norm=None,\n",
        "        fit_y=True,\n",
        "        FIT_METHOD=True,\n",
        "        surrogate=\"GP\",\n",
        "    ):\n",
        "        self.kernel_type = kernel_type\n",
        "        self.bounds_norm = bounds_norm\n",
        "        self.fit_y = fit_y\n",
        "        self.surrogate = surrogate\n",
        "        self.FIT_METHOD = FIT_METHOD\n",
        "        self.scaler_y = TensorStandardScaler()\n",
        "\n",
        "    def fit(self, X_train, y_train):\n",
        "        if type(X_train) == np.ndarray:\n",
        "            X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "\n",
        "        if self.fit_y:\n",
        "            y_train = self.scaler_y.fit_transform(y_train)\n",
        "        else:\n",
        "            y_train = y_train\n",
        "\n",
        "        self.X_train_tensor = torch.tensor(X_train, dtype=torch.float64)\n",
        "        self.y_train_tensor = torch.tensor(y_train, dtype=torch.float64).view(-1, 1)\n",
        "\n",
        "        \"\"\"\n",
        "        Use BoTorch fit method\n",
        "        to fit the hyperparameters of the GP and the model weights\n",
        "        \"\"\"\n",
        "\n",
        "        self.kernel_type == \"Tanimoto\"\n",
        "        kernel = TanimotoKernel()\n",
        "\n",
        "        class InternalGP(SingleTaskGP):\n",
        "            def __init__(self, train_X, train_Y, kernel):\n",
        "                super().__init__(train_X, train_Y)\n",
        "                self.mean_module = ConstantMean()\n",
        "                self.covar_module = ScaleKernel(kernel)\n",
        "\n",
        "        self.gp = InternalGP(self.X_train_tensor, self.y_train_tensor, kernel)\n",
        "\n",
        "        self.gp.likelihood.noise_constraint = gpytorch.constraints.GreaterThan(\n",
        "                1e-3\n",
        "            )\n",
        "\n",
        "        self.mll = ExactMarginalLogLikelihood(self.gp.likelihood, self.gp)\n",
        "        self.mll.to(self.X_train_tensor)\n",
        "\n",
        "        fit_gpytorch_model(self.mll, max_retries=50000)\n",
        "\n",
        "\n",
        "        self.gp.eval()\n",
        "        self.mll.eval()\n",
        "\n",
        "        return self.gp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fEJPdhEEbMS_",
        "outputId": "40e78f8e-1799-4b4c-8831-23abcdc06bd8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "###############################################\n",
            "Entries of X are not between 0 and 1. Adding MinMaxScaler to the pipeline.\n",
            "###############################################\n"
          ]
        }
      ],
      "source": [
        "random.seed(777)\n",
        "np.random.seed(777)\n",
        "\n",
        "def inchi_to_smiles(inchi_list):\n",
        "    \"\"\"\n",
        "    Convert a list of InChI strings to a list of canonical SMILES strings.\n",
        "\n",
        "    Args:\n",
        "    inchi_list (list): A list of InChI strings.\n",
        "\n",
        "    Returns:\n",
        "    list: A list of canonical SMILES strings.\n",
        "    \"\"\"\n",
        "    smiles_list = []\n",
        "    for inchi in inchi_list:\n",
        "        mol = Chem.MolFromInchi(inchi)\n",
        "        if mol:\n",
        "            smiles = Chem.MolToSmiles(mol)\n",
        "            smiles_list.append(smiles)\n",
        "        else:\n",
        "            smiles_list.append(None)  # Append None for invalid InChI strings\n",
        "    return smiles_list\n",
        "\n",
        "\n",
        "class FingerprintGenerator:\n",
        "    def __init__(self, nBits=512, radius=2):\n",
        "        self.nBits = nBits\n",
        "        self.radius = radius\n",
        "\n",
        "    def featurize(self, smiles_list):\n",
        "        fingerprints = []\n",
        "        for smiles in smiles_list:\n",
        "            if not isinstance(smiles, str):\n",
        "                fingerprints.append(np.ones(self.nBits))\n",
        "            else:\n",
        "                mol = Chem.MolFromSmiles(smiles)\n",
        "                if mol is not None:\n",
        "                    fp = AllChem.GetMorganFingerprintAsBitVect(\n",
        "                        mol, self.radius, nBits=self.nBits\n",
        "                    )\n",
        "                    fp_array = np.array(\n",
        "                        list(fp.ToBitString()), dtype=int\n",
        "                    )  # Convert to NumPy array\n",
        "                    fingerprints.append(fp_array)\n",
        "                else:\n",
        "                    print(f\"Could not generate a molecule from SMILES: {smiles}\")\n",
        "                    fingerprints.append(np.array([None]))\n",
        "\n",
        "        return np.array(fingerprints)\n",
        "\n",
        "\n",
        "\n",
        "def convert2pytorch(X, y):\n",
        "    X = torch.from_numpy(X).float()\n",
        "    y = torch.from_numpy(y).float().reshape(-1, 1)\n",
        "    return X, y\n",
        "\n",
        "\n",
        "def check_entries(array_of_arrays):\n",
        "    \"\"\"\n",
        "    Check if the entries of the arrays are between 0 and 1.\n",
        "    Needed for for the datasets.py script.\n",
        "    \"\"\"\n",
        "\n",
        "    for array in array_of_arrays:\n",
        "        for item in array:\n",
        "            if item < 0 or item > 1:\n",
        "                return False\n",
        "    return True\n",
        "\n",
        "\n",
        "class directaryl:\n",
        "    def __init__(self):\n",
        "        # direct arylation reaction\n",
        "        self.ECFP_size = 512\n",
        "        self.radius = 2\n",
        "        self.ftzr = FingerprintGenerator(nBits=self.ECFP_size, radius=self.radius)\n",
        "        dataset_url = \"https://raw.githubusercontent.com/doyle-lab-ucla/edboplus/main/examples/publication/BMS_yield_cost/data/PCI_PMI_cost_full.csv\"\n",
        "        self.data = pd.read_csv(dataset_url)\n",
        "        self.data = self.data.sample(frac=1).reset_index(drop=True)\n",
        "        # create a copy of the data\n",
        "        data_copy = self.data.copy()\n",
        "        # remove the Yield column from the copy\n",
        "        data_copy.drop(\"Yield\", axis=1, inplace=True)\n",
        "        # check for duplicates\n",
        "        duplicates = data_copy.duplicated().any()\n",
        "        if duplicates:\n",
        "            print(\"There are duplicates in the dataset.\")\n",
        "            exit()\n",
        "\n",
        "        self.data[\"Base_SMILES\"] = inchi_to_smiles(self.data[\"Base_inchi\"].values)\n",
        "        self.data[\"Ligand_SMILES\"] = inchi_to_smiles(self.data[\"Ligand_inchi\"].values)\n",
        "        self.data[\"Solvent_SMILES\"] = inchi_to_smiles(self.data[\"Solvent_inchi\"].values)\n",
        "        col_0_base = self.ftzr.featurize(self.data[\"Base_SMILES\"])\n",
        "        col_1_ligand = self.ftzr.featurize(self.data[\"Ligand_SMILES\"])\n",
        "        col_2_solvent = self.ftzr.featurize(self.data[\"Solvent_SMILES\"])\n",
        "        col_3_concentration = self.data[\"Concentration\"].to_numpy().reshape(-1, 1)\n",
        "        col_4_temperature = self.data[\"Temp_C\"].to_numpy().reshape(-1, 1)\n",
        "        self.X = np.concatenate(\n",
        "            [\n",
        "                col_0_base,\n",
        "                col_1_ligand,\n",
        "                col_2_solvent,\n",
        "                col_3_concentration,\n",
        "                col_4_temperature,\n",
        "            ],\n",
        "            axis=1,\n",
        "        )\n",
        "        self.experiments = np.concatenate(\n",
        "            [\n",
        "                self.data[\"Base_SMILES\"].to_numpy().reshape(-1, 1),\n",
        "                self.data[\"Ligand_SMILES\"].to_numpy().reshape(-1, 1),\n",
        "                self.data[\"Solvent_SMILES\"].to_numpy().reshape(-1, 1),\n",
        "                self.data[\"Concentration\"].to_numpy().reshape(-1, 1),\n",
        "                self.data[\"Temp_C\"].to_numpy().reshape(-1, 1),\n",
        "                self.data[\"Yield\"].to_numpy().reshape(-1, 1),\n",
        "            ],\n",
        "            axis=1,\n",
        "        )\n",
        "\n",
        "        self.y = self.data[\"Yield\"].to_numpy()\n",
        "        self.all_ligands = self.data[\"Ligand_SMILES\"].to_numpy()\n",
        "        self.all_bases = self.data[\"Base_SMILES\"].to_numpy()\n",
        "        self.all_solvents = self.data[\"Solvent_SMILES\"].to_numpy()\n",
        "        unique_bases = np.unique(self.data[\"Base_SMILES\"])\n",
        "        unique_ligands = np.unique(self.data[\"Ligand_SMILES\"])\n",
        "        unique_solvents = np.unique(self.data[\"Solvent_SMILES\"])\n",
        "        unique_concentrations = np.unique(self.data[\"Concentration\"])\n",
        "        unique_temperatures = np.unique(self.data[\"Temp_C\"])\n",
        "\n",
        "        max_yield_per_ligand = np.array(\n",
        "            [\n",
        "                max(self.data[self.data[\"Ligand_SMILES\"] == unique_ligand][\"Yield\"])\n",
        "                for unique_ligand in unique_ligands\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        self.worst_ligand = unique_ligands[np.argmin(max_yield_per_ligand)]\n",
        "        self.best_ligand = unique_ligands[np.argmax(max_yield_per_ligand)]\n",
        "\n",
        "        self.where_worst_ligand = np.array(\n",
        "            self.data.index[self.data[\"Ligand_SMILES\"] == self.worst_ligand].tolist()\n",
        "        )\n",
        "\n",
        "        self.feauture_labels = {\n",
        "            \"names\": {\n",
        "                \"bases\": unique_bases,\n",
        "                \"ligands\": unique_ligands,\n",
        "                \"solvents\": unique_solvents,\n",
        "                \"concentrations\": unique_concentrations,\n",
        "                \"temperatures\": unique_temperatures,\n",
        "            },\n",
        "            \"ordered_smiles\": {\n",
        "                \"bases\": self.data[\"Base_SMILES\"],\n",
        "                \"ligands\": self.data[\"Ligand_SMILES\"],\n",
        "                \"solvents\": self.data[\"Solvent_SMILES\"],\n",
        "                \"concentrations\": self.data[\"Concentration\"],\n",
        "                \"temperatures\": self.data[\"Temp_C\"],\n",
        "            },\n",
        "        }\n",
        "\n",
        "\n",
        "class Evaluation_data:\n",
        "    def __init__(\n",
        "        self\n",
        "    ):\n",
        "        self.get_raw_dataset()\n",
        "\n",
        "        rep_size = self.X.shape[1]\n",
        "        self.bounds_norm = torch.tensor([[0] * rep_size, [1] * rep_size])\n",
        "        self.bounds_norm = self.bounds_norm.to(dtype=torch.float32)\n",
        "\n",
        "        if not check_entries(self.X):\n",
        "            print(\"###############################################\")\n",
        "            print(\n",
        "                \"Entries of X are not between 0 and 1. Adding MinMaxScaler to the pipeline.\"\n",
        "            )\n",
        "            print(\"###############################################\")\n",
        "\n",
        "            self.scaler_X = MinMaxScaler()\n",
        "            self.X = self.scaler_X.fit_transform(self.X)\n",
        "\n",
        "    def get_raw_dataset(self):\n",
        "        # https://github.com/doyle-lab-ucla/edboplus/blob/main/examples/publication/BMS_yield_cost/data/PCI_PMI_cost_full_update.csv\n",
        "\n",
        "        BMS = directaryl()\n",
        "        self.data = BMS.data\n",
        "        self.experiments = BMS.experiments\n",
        "        self.X, self.y = BMS.X, BMS.y\n",
        "\n",
        "        self.all_ligands = BMS.all_ligands\n",
        "        self.all_bases = BMS.all_bases\n",
        "        self.all_solvents = BMS.all_solvents\n",
        "\n",
        "        self.best_ligand = BMS.best_ligand\n",
        "        self.worst_ligand = BMS.worst_ligand\n",
        "        self.where_worst_ligand = BMS.where_worst_ligand\n",
        "        self.feauture_labels = BMS.feauture_labels\n",
        "\n",
        "\n",
        "    def get_init_holdout_data(self, SEED):\n",
        "        random.seed(SEED)\n",
        "        torch.manual_seed(SEED)\n",
        "        np.random.seed(SEED)\n",
        "\n",
        "        indices_init = np.random.choice(self.where_worst_ligand[:200], size=48, replace=False)\n",
        "        exp_init = self.experiments[indices_init]\n",
        "        indices_holdout = np.setdiff1d(np.arange(len(self.y)), indices_init)\n",
        "\n",
        "        np.random.shuffle(indices_init)\n",
        "        np.random.shuffle(indices_holdout)\n",
        "\n",
        "        X_init, y_init = self.X[indices_init], self.y[indices_init]\n",
        "        X_holdout, y_holdout = self.X[indices_holdout], self.y[indices_holdout]\n",
        "        exp_holdout = self.experiments[indices_holdout]\n",
        "\n",
        "        LIGANDS_INIT = self.all_ligands[indices_init]\n",
        "        LIGANDS_HOLDOUT = self.all_ligands[indices_holdout]\n",
        "\n",
        "        X_init, y_init = convert2pytorch(X_init, y_init)\n",
        "        X_holdout, y_holdout = convert2pytorch(X_holdout, y_holdout)\n",
        "\n",
        "        return (\n",
        "            X_init,\n",
        "            y_init,\n",
        "            X_holdout,\n",
        "            y_holdout,\n",
        "            LIGANDS_INIT,\n",
        "            LIGANDS_HOLDOUT,\n",
        "            exp_init,\n",
        "            exp_holdout,\n",
        "        )\n",
        "\n",
        "DATASET = Evaluation_data()\n",
        "bounds_norm = DATASET.bounds_norm\n",
        "\n",
        "(\n",
        "    X_init,\n",
        "    y_init,\n",
        "    X_pool_fixed,\n",
        "    y_pool_fixed,\n",
        "    LIGANDS_INIT,\n",
        "    LIGANDS_HOLDOUT,\n",
        "    exp_init,\n",
        "    exp_holdout,\n",
        ") = DATASET.get_init_holdout_data(777)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NUGaPEtHgKj4"
      },
      "source": [
        "![numel.jpeg](data:image/jpeg;base64,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)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "WjqrWY-agK6l"
      },
      "outputs": [],
      "source": [
        "from botorch.acquisition.monte_carlo import qNoisyExpectedImprovement\n",
        "from botorch.optim import optimize_acqf_discrete\n",
        "from botorch.sampling import SobolQMCNormalSampler\n",
        "\n",
        "NUM_RESTARTS = 20\n",
        "RAW_SAMPLES = 512\n",
        "sampler = SobolQMCNormalSampler(1024)\n",
        "\n",
        "\n",
        "def find_indices(X_candidate_BO, candidates):\n",
        "    \"\"\"\n",
        "    Identifies and returns the indices of specific candidates within a larger dataset.\n",
        "    This function is particularly useful when the order of candidates returned by an\n",
        "    acquisition function differs from the original dataset order.\n",
        "\n",
        "    Args:\n",
        "        X_candidate_BO (numpy.ndarray): The complete dataset or holdout set,\n",
        "            typically consisting of feature vectors.\n",
        "        candidates (numpy.ndarray): A subset of the dataset (e.g., a batch of\n",
        "            molecules) selected by the acquisition function.\n",
        "\n",
        "    Returns:\n",
        "        numpy.ndarray: An array of indices corresponding to the positions of\n",
        "            each candidate in the original dataset 'X_candidate_BO'.\n",
        "    \"\"\"\n",
        "\n",
        "    indices = []\n",
        "    for candidate in candidates:\n",
        "        indices.append(np.argwhere((X_candidate_BO == candidate).all(1)).flatten()[0])\n",
        "    indices = np.array(indices)\n",
        "    return indices\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nBpRV_GcuGAf"
      },
      "source": [
        "The main BO loop for fixed `q` and helper functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def bo_inner(model, sampler, bounds_norm, q, \n",
        "             X_train, y_train, X_pool, y_pool, \n",
        "             yield_thr=99.0):    \n",
        "            \n",
        "    # Set up aqf\n",
        "    qNEI = qNoisyExpectedImprovement(model, torch.tensor(X_train), sampler)\n",
        "    X_candidate, _ = optimize_acqf_discrete(\n",
        "      acq_function=qNEI,\n",
        "      bounds=bounds_norm,\n",
        "      q=q,\n",
        "      choices=torch.tensor(X_pool),\n",
        "      unique=True,\n",
        "      num_restarts=NUM_RESTARTS,\n",
        "      raw_samples=RAW_SAMPLES,\n",
        "      sequential=False,\n",
        "    )\n",
        "\n",
        "    # See how they actually look\n",
        "    X_candidate = np.array(X_candidate)\n",
        "    indices = find_indices(X_pool, X_candidate)\n",
        "    indices_keep = np.setdiff1d(np.arange(X_pool.shape[0]), indices)\n",
        "    y_candidate = y_pool[indices]\n",
        "\n",
        "    # We also count the number of experiments conducted\n",
        "    n_experiments = y_candidate.shape[0]\n",
        "\n",
        "    # Remove from pool\n",
        "    X_pool = X_pool[indices_keep]\n",
        "    y_pool = y_pool[indices_keep]\n",
        "\n",
        "    # If we got good performance, we are done\n",
        "    success = any(y_candidate > yield_thr)\n",
        "\n",
        "    if success:\n",
        "      print(\"We found some good candidate! :)\")\n",
        "    else:\n",
        "      print(f\"The best we could do in this selected batch was {max(y_candidate)}! :(\")\n",
        "      X_train = np.vstack([X_train, X_candidate])\n",
        "      y_train = np.concatenate([y_train, y_candidate])\n",
        "      model, _ = update_model(X_train, y_train, bounds_norm, kernel_type=\"Tanimoto\", fit_y=False, FIT_METHOD=True)\n",
        "\n",
        "    print(y_candidate)\n",
        "    return success, n_experiments, model, X_train, y_train, X_pool, y_pool\n",
        "\n",
        " \n",
        "def init_stuff(seed):\n",
        "  # Initialize data from dataset\n",
        "  DATASET = Evaluation_data()\n",
        "  bounds_norm = DATASET.bounds_norm\n",
        "\n",
        "  (\n",
        "      X_init,\n",
        "      y_init,\n",
        "      X_pool_fixed,\n",
        "      y_pool_fixed,\n",
        "      _,\n",
        "      _,\n",
        "      _,\n",
        "      _,\n",
        "  ) = DATASET.get_init_holdout_data(seed)\n",
        "\n",
        "  # Construct initial shitty model\n",
        "  model, _ = update_model(\n",
        "      X_init, y_init, bounds_norm, kernel_type=\"Tanimoto\", fit_y=False, FIT_METHOD=True\n",
        "  )\n",
        "\n",
        "  # Copy things to avoid problems later\n",
        "  X_train = np.copy(X_init)\n",
        "  y_train = np.copy(y_init)\n",
        "  X_pool = np.copy(X_pool_fixed)\n",
        "  y_pool = np.copy(y_pool_fixed)\n",
        "  \n",
        "  return model, X_train, y_train, X_pool, y_pool \n",
        "\n",
        "\n",
        "def bo_above(q, seed, max_iterations=100):\n",
        "\n",
        "  model, X_train, y_train, X_pool, y_pool = init_stuff(seed)\n",
        "\n",
        "  # Count experiments\n",
        "  n_experiments = 0\n",
        "\n",
        "  # Count iterations\n",
        "  n_iter = 0\n",
        "\n",
        "  for i in range(max_iterations):\n",
        "    is_found, n_experiments_incr, model, X_train, y_train, X_pool, y_pool = bo_inner(model, sampler, bounds_norm, q, X_train, y_train, X_pool, y_pool)\n",
        "    n_experiments += n_experiments_incr\n",
        "    n_iter += 1\n",
        "    if is_found is True:\n",
        "      break\n",
        "\n",
        "  return n_experiments, n_iter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The old BO loop to check we don't screw up (and \"tests\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rvJeKzXfuGwj"
      },
      "outputs": [],
      "source": [
        "def bo_above_old(q, seed, max_iterations=100):\n",
        "\n",
        "  # Initialize data from dataset\n",
        "  DATASET = Evaluation_data()\n",
        "  bounds_norm = DATASET.bounds_norm\n",
        "\n",
        "  (\n",
        "      X_init,\n",
        "      y_init,\n",
        "      X_pool_fixed,\n",
        "      y_pool_fixed,\n",
        "      _,\n",
        "      _,\n",
        "      _,\n",
        "      _,\n",
        "  ) = DATASET.get_init_holdout_data(seed)\n",
        "\n",
        "  # Construct initial shitty model\n",
        "  model, _ = update_model(\n",
        "      X_init, y_init, bounds_norm, kernel_type=\"Tanimoto\", fit_y=False, FIT_METHOD=True\n",
        "  )\n",
        "\n",
        "  # Copy things to avoid problems later\n",
        "  X_train = np.copy(X_init)\n",
        "  y_train = np.copy(y_init)\n",
        "  X_pool = np.copy(X_pool_fixed)\n",
        "  y_pool = np.copy(y_pool_fixed)\n",
        "\n",
        "  # Start a timer\n",
        "  fixed_time = 1\n",
        "  overhead_factor = 0.5\n",
        "  overhead_time = overhead_factor * fixed_time\n",
        "  training_time = 2\n",
        "  total_time = 0\n",
        "\n",
        "  # Count experiments\n",
        "  n_experiments = 0\n",
        "\n",
        "  # Count iterations\n",
        "  n_iter = 0\n",
        "\n",
        "  for _ in range(max_iterations):\n",
        "\n",
        "    # Add fixed time\n",
        "    total_time += fixed_time\n",
        "\n",
        "    # Select batch size, for now dummy passed\n",
        "    # Here is where the fun begins! Including the calls below\n",
        "\n",
        "    # Set up aqf\n",
        "    qNEI = qNoisyExpectedImprovement(model, torch.tensor(X_train), sampler)\n",
        "    X_candidate, _ = optimize_acqf_discrete(\n",
        "      acq_function=qNEI,\n",
        "      bounds=bounds_norm,\n",
        "      q=q,\n",
        "      choices=torch.tensor(X_pool),\n",
        "      unique=True,\n",
        "      num_restarts=NUM_RESTARTS,\n",
        "      raw_samples=RAW_SAMPLES,\n",
        "      sequential=False,\n",
        "    )\n",
        "\n",
        "    # See how selected experiments look at the moment\n",
        "    y_pred = model.posterior(X_candidate).mean.detach().flatten().numpy()\n",
        "    y_std = np.sqrt(model.posterior(X_candidate).variance.detach().flatten().numpy())\n",
        "\n",
        "    # See how they actually look\n",
        "    X_candidate = np.array(X_candidate)\n",
        "    indices = find_indices(X_pool, X_candidate)\n",
        "    indices_keep = np.setdiff1d(np.arange(X_pool.shape[0]), indices)\n",
        "    y_candidate = y_pool[indices]\n",
        "\n",
        "    # We have sampled the candidates, so we pay the overhead time n-1 times\n",
        "    total_time += overhead_time*(y_candidate.shape[0] -1)\n",
        "\n",
        "    # We also count the number of experiments conducted\n",
        "    n_experiments += y_candidate.shape[0]\n",
        "    n_iter += 1\n",
        "\n",
        "    # Remove from pool\n",
        "    X_pool = X_pool[indices_keep]\n",
        "    y_pool = y_pool[indices_keep]\n",
        "\n",
        "    # If we got good performance, we are done\n",
        "    if any(y_candidate > 99.0 ): # :)\n",
        "      print(\"We found some good candidate! :)\")\n",
        "      print(y_candidate)\n",
        "      break\n",
        "    else:\n",
        "      print(f\"The best we could do in this selected batch was {max(y_candidate)}! :(\")\n",
        "      print(y_candidate)\n",
        "\n",
        "    # If not, sample points and retrain\n",
        "    X_train = np.vstack([X_train, X_candidate])\n",
        "    y_train = np.concatenate([y_train, y_candidate])\n",
        "    model, _ = update_model(X_train, y_train, bounds_norm, kernel_type=\"Tanimoto\", fit_y=False, FIT_METHOD=True)\n",
        "\n",
        "    # And also add the time it takes to train, which can be important\n",
        "    total_time += training_time\n",
        "\n",
        "  print(f\"The total time for the optimization was {total_time} for a total of {n_experiments} experiments!\")\n",
        "  return n_experiments, n_iter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "bo_above_old(q=3, seed=666, max_iterations=5) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "bo_above(q=3, seed=666, max_iterations=5) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Get baseline results with fixed `q`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cUedzq0ZsIn5",
        "outputId": "2399a0cd-498e-42c6-c720-a75de3a844eb"
      },
      "outputs": [],
      "source": [
        "max_batch_size = 10  # 10\n",
        "n_seeds = 10         # 10\n",
        "max_iterations = 100  # 100\n",
        "\n",
        "q_arr = range(2, max_batch_size+1)\n",
        "\n",
        "timings_all = np.zeros((n_seeds, len(q_arr), 2))\n",
        "for seed in range(n_seeds):\n",
        "  timings_all[seed] = [bo_above(q=q, seed=seed, max_iterations=max_iterations) for q in q_arr]\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dIEEr6AMCj3i"
      },
      "outputs": [],
      "source": [
        "timings_all_mean = timings_all.mean(axis=0)\n",
        "timings_exps = timings_all_mean"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plots\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8vT9AlD3y5Qj"
      },
      "outputs": [],
      "source": [
        "rt_arr = np.linspace(0.1,1.0,5) # time of retraining as % of experiment baseline time\n",
        "ot_arr = np.linspace(0.1,1.0,5) # overhead time per experiment as % of experiment baseline time\n",
        "\n",
        "def compute_cost(rt, ot, n_exp, n_iter):\n",
        "  total_cost = 0.0\n",
        "  total_cost += n_iter  # Baseline experiment cost (per iteration)\n",
        "  total_cost += rt * n_iter # Retraining cost (per iteration)\n",
        "  total_cost += (n_exp - n_iter) * ot # Sum of the overheads\n",
        "  return total_cost\n",
        "\n",
        "\n",
        "def matrix_plot(timings_exps, rt_arr, ot_arr):\n",
        "  x, y = np.meshgrid(rt_arr, ot_arr)\n",
        "  tt_arr = compute_cost(x, y, n_exp, n_iter)\n",
        "  plt.xlabel(\"Retraining cost %\")\n",
        "  plt.ylabel(\"Overhead cost %\")\n",
        "  plt.pcolormesh(rt_arr, ot_arr, tt_arr)\n",
        "  plt.title('Total time as a function of %overhead and %training')\n",
        "  plt.colorbar()\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "  \n",
        "for n_exp, n_iter in timings_exps :\n",
        "  x, y = np.meshgrid(rt_arr, ot_arr)\n",
        "  tt_arr = compute_cost(x, y, n_exp, n_iter)\n",
        "  plt.xlabel(\"Retraining cost %\")\n",
        "  plt.ylabel(\"Overhead cost %\")\n",
        "  plt.pcolormesh(rt_arr, ot_arr, tt_arr)\n",
        "  plt.title('Total time as a function of %overhead and %training')\n",
        "  plt.colorbar()\n",
        "  plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "POyYOhogumLj"
      },
      "outputs": [],
      "source": [
        "z = np.zeros((len(q_arr),25))\n",
        "x, y = np.meshgrid(rt_arr, ot_arr)\n",
        "for i, (n_exp, n_iter) in enumerate(timings_exps) :\n",
        "  p = q_arr\n",
        "  z[i,:] = compute_cost(x,y,n_exp,n_iter).flatten()\n",
        "plt.plot(p,z)\n",
        "plt.title('Total time as a function of q')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "BO loop but with `q` depending on iteration number"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def bo_above_flex_batch(q_arr, seed, max_iterations=100):\n",
        "\n",
        "  model, X_train, y_train, X_pool, y_pool = init_stuff(seed)\n",
        "\n",
        "  # Count experiments\n",
        "  n_experiments = 0\n",
        "  \n",
        "  for i in range(max_iterations):\n",
        "    q = q_arr[i] if i<len(q_arr) else q_arr[-1]\n",
        "    is_found, n_experiments_incr, model, X_train, y_train, X_pool, y_pool = bo_inner(model, sampler, bounds_norm, q, X_train, y_train, X_pool, y_pool)\n",
        "    n_experiments += n_experiments_incr\n",
        "    if is_found is True:\n",
        "      break\n",
        "\n",
        "  return n_experiments, i+1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Try different ways to change `q` "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "###############################################\n",
            "Entries of X are not between 0 and 1. Adding MinMaxScaler to the pipeline.\n",
            "###############################################\n",
            "The best we could do in this selected batch was [20.67]! :(\n",
            "[[ 0.  ]\n",
            " [ 7.12]\n",
            " [ 4.98]\n",
            " [20.67]\n",
            " [ 0.41]]\n",
            "The best we could do in this selected batch was [44.47]! :(\n",
            "[[16.15]\n",
            " [16.95]\n",
            " [13.02]\n",
            " [44.47]]\n",
            "We found some good candidate! :)\n",
            "[[80.65]\n",
            " [99.81]\n",
            " [23.8 ]]\n",
            "###############################################\n",
            "Entries of X are not between 0 and 1. Adding MinMaxScaler to the pipeline.\n",
            "###############################################\n",
            "The best we could do in this selected batch was [29.23]! :(\n",
            "[[ 0.  ]\n",
            " [ 3.79]\n",
            " [21.5 ]\n",
            " [29.23]\n",
            " [ 0.4 ]]\n",
            "The best we could do in this selected batch was [58.13]! :(\n",
            "[[28.17]\n",
            " [18.19]\n",
            " [21.15]\n",
            " [58.13]]\n",
            "The best we could do in this selected batch was [27.88]! :(\n",
            "[[27.88]\n",
            " [ 3.69]\n",
            " [13.52]]\n",
            "The best we could do in this selected batch was [42.63]! :(\n",
            "[[42.63]\n",
            " [31.51]]\n",
            "The best we could do in this selected batch was [4.78]! :(\n",
            "[[4.78]\n",
            " [3.14]]\n",
            "The best we could do in this selected batch was [66.84]! :(\n",
            "[[25.45]\n",
            " [66.84]]\n",
            "The best we could do in this selected batch was [88.89]! :(\n",
            "[[88.89]\n",
            " [ 7.42]]\n",
            "The best we could do in this selected batch was [76.59]! :(\n",
            "[[55.96]\n",
            " [76.59]]\n",
            "The best we could do in this selected batch was [68.71]! :(\n",
            "[[57.27]\n",
            " [68.71]]\n",
            "The best we could do in this selected batch was [62.04]! :(\n",
            "[[62.04]\n",
            " [ 6.22]]\n",
            "The best we could do in this selected batch was [96.64]! :(\n",
            "[[ 2.13]\n",
            " [96.64]]\n",
            "The best we could do in this selected batch was [98.38]! :(\n",
            "[[98.38]\n",
            " [95.48]]\n",
            "We found some good candidate! :)\n",
            "[[99.22]\n",
            " [99.81]]\n",
            "###############################################\n",
            "Entries of X are not between 0 and 1. Adding MinMaxScaler to the pipeline.\n",
            "###############################################\n",
            "The best we could do in this selected batch was [95.48]! :(\n",
            "[[89.71]\n",
            " [35.61]\n",
            " [78.6 ]\n",
            " [95.48]\n",
            " [81.48]]\n",
            "We found some good candidate! :)\n",
            "[[96.64]\n",
            " [38.8 ]\n",
            " [99.22]\n",
            " [92.03]]\n",
            "###############################################\n",
            "Entries of X are not between 0 and 1. Adding MinMaxScaler to the pipeline.\n",
            "###############################################\n",
            "The best we could do in this selected batch was [89.71]! :(\n",
            "[[89.71]\n",
            " [35.61]\n",
            " [18.16]\n",
            " [29.06]\n",
            " [ 1.62]]\n",
            "The best we could do in this selected batch was [87.32]! :(\n",
            "[[53.46]\n",
            " [87.32]\n",
            " [36.96]\n",
            " [62.82]]\n",
            "The best we could do in this selected batch was [81.48]! :(\n",
            "[[78.19]\n",
            " [16.77]\n",
            " [81.48]]\n",
            "The best we could do in this selected batch was [82.13]! :(\n",
            "[[ 6.44]\n",
            " [82.13]]\n",
            "The best we could do in this selected batch was [89.14]! :(\n",
            "[[22.07]\n",
            " [89.14]]\n",
            "The best we could do in this selected batch was [85.21]! :(\n",
            "[[ 3.14]\n",
            " [85.21]]\n",
            "The best we could do in this selected batch was [88.89]! :(\n",
            "[[ 5.16]\n",
            " [88.89]]\n",
            "The best we could do in this selected batch was [55.96]! :(\n",
            "[[55.96]\n",
            " [ 8.52]]\n",
            "The best we could do in this selected batch was [96.64]! :(\n",
            "[[96.64]\n",
            " [66.84]]\n",
            "The best we could do in this selected batch was [89.95]! :(\n",
            "[[89.95]\n",
            " [31.14]]\n",
            "The best we could do in this selected batch was [98.38]! :(\n",
            "[[95.48]\n",
            " [98.38]]\n",
            "We found some good candidate! :)\n",
            "[[99.22]\n",
            " [52.47]]\n",
            "###############################################\n",
            "Entries of X are not between 0 and 1. Adding MinMaxScaler to the pipeline.\n",
            "###############################################\n",
            "The best we could do in this selected batch was [72.24]! :(\n",
            "[[ 0.  ]\n",
            " [72.24]\n",
            " [ 3.06]\n",
            " [ 0.28]\n",
            " [ 8.45]]\n",
            "The best we could do in this selected batch was [89.14]! :(\n",
            "[[40.47]\n",
            " [36.97]\n",
            " [89.14]\n",
            " [ 7.12]]\n",
            "The best we could do in this selected batch was [21.88]! :(\n",
            "[[ 5.4 ]\n",
            " [21.88]\n",
            " [ 5.16]]\n",
            "The best we could do in this selected batch was [62.04]! :(\n",
            "[[ 3.14]\n",
            " [62.04]]\n",
            "The best we could do in this selected batch was [88.89]! :(\n",
            "[[88.89]\n",
            " [ 2.13]]\n",
            "The best we could do in this selected batch was [57.27]! :(\n",
            "[[ 8.52]\n",
            " [57.27]]\n",
            "The best we could do in this selected batch was [66.84]! :(\n",
            "[[66.84]\n",
            " [55.96]]\n",
            "The best we could do in this selected batch was [96.64]! :(\n",
            "[[18.38]\n",
            " [96.64]]\n",
            "The best we could do in this selected batch was [95.48]! :(\n",
            "[[95.48]\n",
            " [89.95]]\n",
            "The best we could do in this selected batch was [98.38]! :(\n",
            "[[98.38]\n",
            " [92.03]]\n",
            "We found some good candidate! :)\n",
            "[[99.22]\n",
            " [62.33]]\n",
            "###############################################\n",
            "Entries of X are not between 0 and 1. Adding MinMaxScaler to the pipeline.\n",
            "###############################################\n",
            "The best we could do in this selected batch was [20.5]! :(\n",
            "[[ 0.  ]\n",
            " [20.5 ]\n",
            " [ 5.85]\n",
            " [ 7.59]\n",
            " [ 8.45]]\n",
            "The best we could do in this selected batch was [89.14]! :(\n",
            "[[89.14]\n",
            " [16.57]\n",
            " [23.51]\n",
            " [ 3.79]]\n",
            "The best we could do in this selected batch was [85.21]! :(\n",
            "[[81.48]\n",
            " [85.21]\n",
            " [39.57]]\n",
            "The best we could do in this selected batch was [72.24]! :(\n",
            "[[ 3.14]\n",
            " [72.24]]\n",
            "The best we could do in this selected batch was [88.89]! :(\n",
            "[[82.13]\n",
            " [88.89]]\n",
            "The best we could do in this selected batch was [66.84]! :(\n",
            "[[55.96]\n",
            " [66.84]]\n",
            "The best we could do in this selected batch was [57.27]! :(\n",
            "[[57.27]\n",
            " [ 6.22]]\n",
            "The best we could do in this selected batch was [62.04]! :(\n",
            "[[62.04]\n",
            " [ 2.13]]\n",
            "The best we could do in this selected batch was [96.64]! :(\n",
            "[[76.59]\n",
            " [96.64]]\n",
            "The best we could do in this selected batch was [95.48]! :(\n",
            "[[89.95]\n",
            " [95.48]]\n",
            "The best we could do in this selected batch was [98.38]! :(\n",
            "[[98.38]\n",
            " [92.03]]\n",
            "We found some good candidate! :)\n",
            "[[99.22]\n",
            " [30.41]]\n",
            "###############################################\n",
            "Entries of X are not between 0 and 1. Adding MinMaxScaler to the pipeline.\n",
            "###############################################\n",
            "The best we could do in this selected batch was [44.22]! :(\n",
            "[[ 0.  ]\n",
            " [20.5 ]\n",
            " [ 3.06]\n",
            " [ 0.  ]\n",
            " [44.22]]\n",
            "The best we could do in this selected batch was [95.48]! :(\n",
            "[[59.82]\n",
            " [11.95]\n",
            " [95.48]\n",
            " [28.4 ]]\n",
            "We found some good candidate! :)\n",
            "[[99.22]\n",
            " [92.19]\n",
            " [99.98]]\n",
            "###############################################\n",
            "Entries of X are not between 0 and 1. Adding MinMaxScaler to the pipeline.\n",
            "###############################################\n",
            "The best we could do in this selected batch was [40.47]! :(\n",
            "[[ 8.04]\n",
            " [12.39]\n",
            " [ 0.  ]\n",
            " [15.92]\n",
            " [40.47]]\n",
            "The best we could do in this selected batch was [81.48]! :(\n",
            "[[11.77]\n",
            " [81.48]\n",
            " [72.24]\n",
            " [ 2.72]]\n",
            "The best we could do in this selected batch was [89.14]! :(\n",
            "[[89.14]\n",
            " [ 5.4 ]\n",
            " [ 5.16]]\n",
            "The best we could do in this selected batch was [77.58]! :(\n",
            "[[ 3.14]\n",
            " [77.58]]\n",
            "The best we could do in this selected batch was [88.89]! :(\n",
            "[[88.89]\n",
            " [45.62]]\n",
            "The best we could do in this selected batch was [96.64]! :(\n",
            "[[96.64]\n",
            " [ 8.52]]\n",
            "The best we could do in this selected batch was [95.48]! :(\n",
            "[[95.48]\n",
            " [31.14]]\n",
            "The best we could do in this selected batch was [89.95]! :(\n",
            "[[89.95]\n",
            " [62.04]]\n",
            "The best we could do in this selected batch was [30.41]! :(\n",
            "[[30.41]\n",
            " [ 2.13]]\n",
            "The best we could do in this selected batch was [16.77]! :(\n",
            "[[16.77]\n",
            " [14.24]]\n",
            "The best we could do in this selected batch was [98.38]! :(\n",
            "[[46.42]\n",
            " [98.38]]\n",
            "We found some good candidate! :)\n",
            "[[99.98]\n",
            " [ 0.  ]]\n",
            "###############################################\n",
            "Entries of X are not between 0 and 1. Adding MinMaxScaler to the pipeline.\n",
            "###############################################\n",
            "The best we could do in this selected batch was [20.5]! :(\n",
            "[[ 0.  ]\n",
            " [20.5 ]\n",
            " [ 0.28]\n",
            " [ 0.4 ]\n",
            " [ 3.06]]\n",
            "The best we could do in this selected batch was [23.51]! :(\n",
            "[[20.72]\n",
            " [23.51]\n",
            " [16.57]\n",
            " [21.88]]\n",
            "The best we could do in this selected batch was [81.48]! :(\n",
            "[[23.23]\n",
            " [81.48]\n",
            " [ 6.44]]\n",
            "The best we could do in this selected batch was [40.47]! :(\n",
            "[[40.47]\n",
            " [39.57]]\n",
            "The best we could do in this selected batch was [89.14]! :(\n",
            "[[89.14]\n",
            " [22.07]]\n",
            "The best we could do in this selected batch was [77.58]! :(\n",
            "[[ 3.14]\n",
            " [77.58]]\n",
            "The best we could do in this selected batch was [88.89]! :(\n",
            "[[88.89]\n",
            " [61.86]]\n",
            "The best we could do in this selected batch was [66.84]! :(\n",
            "[[62.04]\n",
            " [66.84]]\n",
            "The best we could do in this selected batch was [6.22]! :(\n",
            "[[6.22]\n",
            " [0.15]]\n",
            "The best we could do in this selected batch was [96.64]! :(\n",
            "[[96.64]\n",
            " [ 2.13]]\n",
            "The best we could do in this selected batch was [95.48]! :(\n",
            "[[95.48]\n",
            " [18.38]]\n",
            "The best we could do in this selected batch was [98.38]! :(\n",
            "[[46.42]\n",
            " [98.38]]\n",
            "We found some good candidate! :)\n",
            "[[99.81]\n",
            " [80.86]]\n",
            "###############################################\n",
            "Entries of X are not between 0 and 1. Adding MinMaxScaler to the pipeline.\n",
            "###############################################\n",
            "The best we could do in this selected batch was [59.82]! :(\n",
            "[[ 0.  ]\n",
            " [ 5.48]\n",
            " [ 0.14]\n",
            " [ 0.  ]\n",
            " [59.82]]\n",
            "The best we could do in this selected batch was [91.19]! :(\n",
            "[[28.4 ]\n",
            " [31.14]\n",
            " [91.19]\n",
            " [44.22]]\n",
            "We found some good candidate! :)\n",
            "[[99.22]\n",
            " [19.13]\n",
            " [99.98]]\n",
            "[[12.  3.]\n",
            " [32. 13.]\n",
            " [ 9.  2.]\n",
            " [30. 12.]\n",
            " [28. 11.]\n",
            " [30. 12.]\n",
            " [12.  3.]\n",
            " [30. 12.]\n",
            " [32. 13.]\n",
            " [12.  3.]]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# q_arr = np.arange(10,1,-2)\n",
        "# q_arr = np.arange(7,1,-1)\n",
        "q_arr = np.arange(5,1,-1)\n",
        "\n",
        "n_seeds = 10         # 10\n",
        "max_iterations = 100  # 100\n",
        "\n",
        "timings_all = np.zeros((n_seeds, 2))\n",
        "for seed in range(n_seeds):\n",
        "  timings_all[seed] = bo_above_flex_batch(q_arr, seed=seed, max_iterations=max_iterations)\n",
        "\n",
        "print(timings_all)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(array([5, 4, 3, 2]),\n",
              " array([[12.,  3.],\n",
              "        [32., 13.],\n",
              "        [ 9.,  2.],\n",
              "        [30., 12.],\n",
              "        [28., 11.],\n",
              "        [30., 12.],\n",
              "        [12.,  3.],\n",
              "        [30., 12.],\n",
              "        [32., 13.],\n",
              "        [12.,  3.]]))"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "q_arr, timings_all\n",
        "#1st column is the number of experiments, 2nd column is the number of iterations\n",
        "#what is the targe value for the number of experiments? 99.0"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
